{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Nj76qUe2lnA"
   },
   "source": [
    "## [DACON] 진동데이터 활용 충돌체 탐지 AI 경진대회\n",
    "## 1Gb (팀명) (Team name)\n",
    "## 2020년 월 일 (제출날짜) (Submission date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYHb_Mf-2lnG"
   },
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtKnyRor2lnI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten,MaxPooling2D,BatchNormalization,Lambda, AveragePooling2D, Input, Conv1D, Multiply, Add, Dropout, GlobalMaxPooling2D, GlobalAvgPool2D\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LambdaCallback\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zjQY_KY2lnR"
   },
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb0OD3v82lnT"
   },
   "outputs": [],
   "source": [
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    \n",
    "    signal_col = [f'S{x}' for x in range(1, 5)]\n",
    "    for col in signal_col:\n",
    "        train_input_mean = train[col].mean()\n",
    "        train_input_sigma = train[col].std()\n",
    "        train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
    "        test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/KAERI_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_itg(signal_feat):\n",
    "    itg = np.trapz(signal_feat.values, axis=0)\n",
    "    return itg\n",
    "\n",
    "def rolling_features(train, test):\n",
    "    \n",
    "    pre_train = train.copy()\n",
    "    pre_test = test.copy()\n",
    "    \n",
    "        \n",
    "    for df in [pre_train, pre_test]:\n",
    "                \n",
    "        for window in [50]:\n",
    "            \n",
    "            # roll backwards\n",
    "            df['S1mean_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "            df['S1mean_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "            df['S1mean_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "            df['S1mean_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "\n",
    "            df['S1var_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "            df['S1var_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "            df['S1var_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "            df['S1var_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "\n",
    "            df['S1min_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            df['S1min_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            df['S1min_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            df['S1min_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            \n",
    "            \n",
    "            df['S1max_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "            df['S1max_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "            df['S1max_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "            df['S1max_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "            \n",
    "            min_max = (df['S1'] - df['S1min_t' + str(window) + '_S1']) / (df['S1max_t' + str(window) + '_S1'] - df['S1min_t' + str(window) + '_S1'])\n",
    "            df['norm_t' + str(window) + '_S1'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S1']) - np.ceil(df['S1min_t' + str(window) + '_S1']))\n",
    "            min_max = (df['S2'] - df['S1min_t' + str(window) + '_S2']) / (df['S1max_t' + str(window) + '_S2'] - df['S1min_t' + str(window) + '_S2'])\n",
    "            df['norm_t' + str(window) + '_S2'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S2']) - np.ceil(df['S1min_t' + str(window) + '_S2']))\n",
    "            min_max = (df['S3'] - df['S1min_t' + str(window) + '_S3']) / (df['S1max_t' + str(window) + '_S3'] - df['S1min_t' + str(window) + '_S3'])\n",
    "            df['norm_t' + str(window) + '_S3'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S3']) - np.ceil(df['S1min_t' + str(window) + '_S3']))\n",
    "            min_max = (df['S4'] - df['S1min_t' + str(window) + '_S4']) / (df['S1max_t' + str(window) + '_S4'] - df['S1min_t' + str(window) + '_S4'])\n",
    "            df['norm_t' + str(window) + '_S4'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S4']) - np.ceil(df['S1min_t' + str(window) + '_S4']))\n",
    "\n",
    "    del train, test, min_max\n",
    "    \n",
    "    \n",
    "    return pre_train.fillna(0), pre_test.fillna(0)\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    \n",
    "    signal_col = [f'S{x}' for x in range(1, 5)]\n",
    "    for col in signal_col:\n",
    "        for window in windows:\n",
    "            df[col + '_pos_' + str(window)] = df.groupby('id')[col].shift(window).fillna(0)\n",
    "            df[col + '_shift_neg_' + str(window)] = df.groupby('id')[col].shift(-1 * window).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_cusum(df):\n",
    "    signal_col = [f'S{x}' for x in range(1, 5)]\n",
    "    for col in signal_col:\n",
    "        df[col + '_cum_sum'] = df.groupby('id')[col].cumsum().fillna(0)\n",
    "    return df\n",
    "\n",
    "def com_gradient(signal_faet, step=1):\n",
    "    g = signal_faet\n",
    "    for _ in range(step):\n",
    "        g = np.gradient(g)\n",
    "    return g\n",
    "\n",
    "def get_gradient(df, step=1):\n",
    "    signal_col = [f'S{x}' for x in range(1, 5)]\n",
    "    for col in signal_col:\n",
    "        df[col + '_g_' + f'{step}'] = np.concatenate(df.groupby('id')['S1'].apply(lambda x : com_gradient(x.values, 1)).values)\n",
    "\n",
    "    return df\n",
    "\n",
    "def com_peak(signal_feat):\n",
    "    '''\n",
    "    signal_feat -> B, S, F\n",
    "    B : number of data\n",
    "    S : sequncer size\n",
    "    F : number of feature\n",
    "    '''\n",
    "    peaks = np.zeros_like(signal_feat)\n",
    "    \n",
    "    # find peak\n",
    "    peak_1, _ = find_peaks(signal_feat)\n",
    "    peak_2, _ = find_peaks(-signal_feat)\n",
    "    peaks[peak_1] = 1\n",
    "    peaks[peak_2] = -1\n",
    "    \n",
    "    return peaks\n",
    "\n",
    "def get_peak(df):\n",
    "    signal_col = [f'S{x}' for x in range(1, 5)]\n",
    "    for col in signal_col:\n",
    "        df[col + '_peak'] = np.concatenate(df.groupby('id')[col].apply(lambda x : com_peak(x)).values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df):\n",
    "\n",
    "    # create leads and lags (1, 2, 3 making them 6 features)\n",
    "    df = lag_with_pct_change(df, [1, 2])\n",
    "    # cumerate sum\n",
    "    df = get_cusum(df)\n",
    "    # get gradient\n",
    "#     df = get_gradient(df, 1)\n",
    "#     df = get_gradient(df, 2)\n",
    "    # get peak\n",
    "    df = get_peak(df)\n",
    "    \n",
    "    return df\n",
    "def create_feature():\n",
    "    train = pd.read_csv(DATA_PATH + '/train_features.csv')\n",
    "    test = pd.read_csv(DATA_PATH + '/test_features.csv')\n",
    "    \n",
    "    train, test = normalize(train, test)\n",
    "    train, test = rolling_features(train, test)\n",
    "    \n",
    "    train = run_feat_engineering(train)\n",
    "    test = run_feat_engineering(test)\n",
    "    feture_dir = './new_feature'\n",
    "\n",
    "    if not os.path.exists(feture_dir):\n",
    "        os.mkdir(feture_dir)\n",
    "    train.to_csv(feture_dir+'/new_feat_train_V6.csv',index=False)\n",
    "    test.to_csv(feture_dir+'/new_feat_test_V6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original data\n",
    "X_data = pd.read_csv(DATA_PATH+'/train_features.csv')\n",
    "X_data_test = pd.read_csv(DATA_PATH+'/test_features.csv')\n",
    "\n",
    "\n",
    "# Normalize train test\n",
    "X_data, X_data_test = normalize(X_data, X_data_test)\n",
    "\n",
    "# Drop unnecessary colunm\n",
    "X_data = X_data.drop(columns=['id'], axis=1).values\n",
    "X_data_test = X_data_test.drop(columns=['id'], axis=1).values\n",
    "\n",
    "# Resize\n",
    "X_data = X_data.reshape((2800,375,5,1))\n",
    "X_data_test = X_data_test.reshape((700,375,5,1))\n",
    "\n",
    "# Check shape\n",
    "print(X_data.shape)\n",
    "print(X_data_test.shape)\n",
    "\n",
    "# Read Target value\n",
    "Y_data = np.loadtxt(DATA_PATH+'/train_target.csv',skiprows=1,delimiter=',')\n",
    "Y_data = Y_data[:,1:]\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Feature directory\n",
    "FEAT_PATH = './new_feature'\n",
    "\n",
    "# X_new_feat_trn = np.loadtxt(FEAT_PATH+'/new_feat_train_V2.csv',skiprows=1,delimiter=',')\n",
    "X_new_feat_trn = pd.read_csv(FEAT_PATH+'/new_feat_train_V6.csv')\n",
    "X_new_feat_trn = X_new_feat_trn.drop(columns=['id'], axis=1).values\n",
    "X_new_feat_trn = X_new_feat_trn.reshape((2800,375,50,1))\n",
    "print(X_new_feat_trn.shape)\n",
    "\n",
    "\n",
    "# # X_new_feat_test = np.loadtxt(FEAT_PATH+'/new_feat_test_V2.csv',skiprows=1,delimiter=',')\n",
    "X_new_feat_test = pd.read_csv(FEAT_PATH+'/new_feat_test_V6.csv')\n",
    "X_new_feat_test = X_new_feat_test.drop(columns=['id'], axis=1).values\n",
    "X_new_feat_test = X_new_feat_test.reshape((700,375,50,1))\n",
    "print(X_new_feat_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qyq90ZzB2lnk"
   },
   "source": [
    "## 3. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1 = np.array([1,1,0,0])\n",
    "weight2 = np.array([0,0,1,1])\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*weight1)/2e+04\n",
    "\n",
    "def my_loss_E2(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaeri_metric(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: KAERI metric\n",
    "    '''\n",
    "    \n",
    "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
    "\n",
    "\n",
    "### E1과 E2는 아래에 정의됨 ###\n",
    "\n",
    "def E1(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: distance error normalized with 2e+04\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
    "    \n",
    "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
    "\n",
    "\n",
    "def E2(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))\n",
    "\n",
    "def E2M(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,2], np.array(y_pred)[:,2]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))\n",
    "\n",
    "def E2V(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,-1], np.array(y_pred)[:,-1]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rel1AkRP2lnm"
   },
   "outputs": [],
   "source": [
    "def set_model(train_target):  # 0:x,y, 1:m, 2:v\n",
    "    \n",
    "    activation = 'elu'\n",
    "    padding = 'valid'\n",
    "    model = Sequential()\n",
    "    nf = do_nf\n",
    "    fs = (3,1)\n",
    "    \n",
    "    if train_target == 2:\n",
    "        if do_slice:\n",
    "            model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(200,5,1)))\n",
    "        else:\n",
    "            model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(375,5,1)))\n",
    "    else:\n",
    "        if do_slice:\n",
    "            model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(200,50,1)))\n",
    "        else:\n",
    "            model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(375,50,1)))\n",
    "            \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "    \n",
    "    if do_gap:\n",
    "        if train_target == 0:\n",
    "            model.add(Flatten())\n",
    "        else:\n",
    "            model.add(GlobalAvgPool2D())\n",
    "    else:\n",
    "        model.add(Flatten())\n",
    "    model.add(Dense(128, activation ='elu'))\n",
    "    model.add(Dense(64, activation ='elu'))\n",
    "    model.add(Dense(32, activation ='elu'))\n",
    "    model.add(Dense(16, activation ='elu'))\n",
    "    model.add(Dense(4))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    global weight2\n",
    "    if train_target == 1: # only for M\n",
    "        weight2 = np.array([0,0,1,0])\n",
    "    else: # only for V\n",
    "        weight2 = np.array([0,0,0,1])\n",
    "       \n",
    "    \n",
    "    if train_target==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "       \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V21KUtyl2lnu"
   },
   "source": [
    "## 4. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNU-ahud2lnv"
   },
   "outputs": [],
   "source": [
    "def train(model,X,Y, split, train_target):\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "    \n",
    "    trn_id, val_id = split[0], split[1]\n",
    "\n",
    "    best_save = ModelCheckpoint('./model/'+'best_m.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, min_lr=0.00001)\n",
    "    \n",
    "    callbacks = list()\n",
    "    callbacks.append(best_save)\n",
    "    callbacks.append(reduce_lr)\n",
    "    \n",
    "    trn_X, trn_Y = X[trn_id], Y[trn_id]\n",
    "    val_X, val_Y = X[val_id], Y[val_id]\n",
    "    \n",
    "    if train_target == 2:\n",
    "        \n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=500,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(val_X, val_Y),\n",
    "                      verbose = 2,\n",
    "                      callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=300,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(val_X, val_Y),\n",
    "                      verbose = 2,\n",
    "                      callbacks=callbacks)\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "    loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "    loss_ax.axvline(x=np.argmin(history.history['val_loss']), color='b', linewidth=1)\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "    plt.show()    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(train_target, train_X, train_Y, train_new_feat_X):\n",
    "\n",
    "    if train_target == 0:\n",
    "        model = load_model('./model/best_m.hdf5' , custom_objects={'my_loss_E1': my_loss, })\n",
    "    else:\n",
    "        model = load_model('./model/best_m.hdf5' , custom_objects={'my_loss_E2': my_loss, })\n",
    "  \n",
    "    if train_target == 2:\n",
    "        pred = model.predict(train_X)\n",
    "    else:\n",
    "        pred = model.predict(train_new_feat_X)\n",
    "    \n",
    "    i=0\n",
    "    print('정답(original):', train_Y[i])\n",
    "    print('예측값(original):', pred[i])\n",
    "    e1_score = E1(train_Y, pred)\n",
    "    e2_score = E2(train_Y, pred)\n",
    "    e2m_score = E2M(train_Y, pred)\n",
    "    e2v_score = E2V(train_Y, pred)\n",
    "    \n",
    "    print('E1 : ', e1_score)\n",
    "    print('E2 : ', e2_score)\n",
    "    print('E2M : ', e2m_score)\n",
    "    print('E2V : ', e2v_score)    \n",
    "    \n",
    "    \n",
    "    return model, [e1_score, e2_score, e2m_score, e2v_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test):\n",
    "    submit = pd.read_csv('../data/KAERI_dataset/sample_submission.csv')\n",
    "    \n",
    "    if do_slice:\n",
    "        d_X_data = X_data[:, :200]\n",
    "        d_X_data_test = X_data_test[:, :200]\n",
    "\n",
    "        d_X_new_feat_trn = X_new_feat_trn[:, :200]\n",
    "        d_X_new_feat_test = X_new_feat_test[:, :200]\n",
    "    else:\n",
    "        d_X_data = X_data\n",
    "        d_X_data_test = X_data_test\n",
    "\n",
    "        d_X_new_feat_trn = X_new_feat_trn\n",
    "        d_X_new_feat_test = X_new_feat_test\n",
    "\n",
    "    if do_flip:\n",
    "        d_X_data = np.flip(d_X_data, axis=1)\n",
    "        d_X_data_test = np.flip(d_X_data_test, axis=1)\n",
    "\n",
    "        d_X_new_feat_trn = np.flip(d_X_new_feat_trn, axis=1)\n",
    "        d_X_new_feat_test = np.flip(d_X_new_feat_test, axis=1)\n",
    "        \n",
    "        \n",
    "    kfold = KFold(n_splits=n_fold, shuffle=False, random_state=42)\n",
    "    splits = [x for x in kfold.split(X_data, Y_data)]\n",
    "    \n",
    "    score_list = list()\n",
    "    for train_target in range(3):\n",
    "\n",
    "        for split in splits:\n",
    "            model = set_model(train_target)\n",
    "\n",
    "            if train_target == 2:\n",
    "                train(model, d_X_data, Y_data, split, train_target)\n",
    "            else:\n",
    "                train(model, d_X_new_feat_trn, Y_data, split, train_target)\n",
    "\n",
    "            best_model, scores = load_best_model(train_target, d_X_data, Y_data, d_X_new_feat_trn)\n",
    "\n",
    "            if train_target == 2:\n",
    "                pred_data_test = best_model.predict(d_X_data_test)\n",
    "            else:\n",
    "                pred_data_test = best_model.predict(d_X_new_feat_test)\n",
    "\n",
    "            score_list.append(scores)\n",
    "\n",
    "            if train_target == 0: # x,y 학습\n",
    "                submit.iloc[:,1] += pred_data_test[:,0] / n_fold\n",
    "                submit.iloc[:,2] += pred_data_test[:,1] / n_fold\n",
    "\n",
    "            elif train_target == 1: # m 학습\n",
    "                submit.iloc[:,3] += pred_data_test[:,2] / n_fold\n",
    "\n",
    "            elif train_target == 2: # v 학습\n",
    "                submit.iloc[:,4] += pred_data_test[:,3] / n_fold\n",
    "\n",
    "    submit.to_csv(pred_name, index=False)\n",
    "    print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_naming(do_nf, do_gap, do_flip, do_slice):\n",
    "    pred_name = './cnn'\n",
    "    pred_name += f'_nf{do_nf}'\n",
    "    if do_gap:\n",
    "        pred_name += '_gap'\n",
    "    if do_flip:\n",
    "        pred_name += '_flip'\n",
    "    if not do_slice:\n",
    "        pred_name += '_375'\n",
    "\n",
    "    pred_name += '_pred.csv'\n",
    "    \n",
    "    print(pred_name)\n",
    "    \n",
    "    return pred_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 1\n",
    "do_slice=True\n",
    "do_gap=False\n",
    "do_nf=32\n",
    "do_flip=False\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2\n",
    "do_slice=True\n",
    "do_gap=False\n",
    "do_nf=32\n",
    "do_flip=True\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 3\n",
    "do_slice=True\n",
    "do_gap=False\n",
    "do_nf=16\n",
    "do_flip=False\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 4\n",
    "do_slice=True\n",
    "do_gap=False\n",
    "do_nf=16\n",
    "do_flip=True\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 5\n",
    "do_slice=True\n",
    "do_gap=True\n",
    "do_nf=16\n",
    "do_flip=False\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 6\n",
    "do_slice=True\n",
    "do_gap=True\n",
    "do_nf=16\n",
    "do_flip=True\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 7\n",
    "do_slice=True\n",
    "do_gap=True\n",
    "do_nf=32\n",
    "do_flip=False\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 8\n",
    "do_slice=True\n",
    "do_gap=True\n",
    "do_nf=32\n",
    "do_flip=True\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 9\n",
    "do_slice=False\n",
    "do_gap=False\n",
    "do_nf=16\n",
    "do_flip=False\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 10\n",
    "do_slice=False\n",
    "do_gap=True\n",
    "do_nf=16\n",
    "do_flip=False\n",
    "n_fold = 5\n",
    "\n",
    "pred_name = pred_naming(do_nf, do_gap, do_flip, do_slice)\n",
    "do_train(do_slice, do_flip, pred_name, X_new_feat_trn, X_new_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KAERI_source_code.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
