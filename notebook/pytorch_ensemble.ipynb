{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Nj76qUe2lnA"
   },
   "source": [
    "## [DACON] 진동데이터 활용 충돌체 탐지 AI 경진대회\n",
    "## 1Gb (팀명) (Team name)\n",
    "## 2020년 월 일 (제출날짜) (Submission date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYHb_Mf-2lnG"
   },
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtKnyRor2lnI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from losses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBS = 375\n",
    "NNBATCHSIZE = 256\n",
    "SPLIT = 5\n",
    "LR = 0.0015\n",
    "EPOCHS = 600\n",
    "SEED = 62\n",
    "\n",
    "outdir = 'wavenet_models'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zjQY_KY2lnR"
   },
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb0OD3v82lnT"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/KAERI_dataset/train_features.csv')\n",
    "test = pd.read_csv('./data/KAERI_dataset/test_features.csv')\n",
    "train_y = pd.read_csv('./data/KAERI_dataset/train_target.csv')\n",
    "\n",
    "train = train.merge(train_y, on='id', how='left')\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(df, batch_size):\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['S1'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "# rolling and aggreagate batch features\n",
    "def rolling_features(train, test):\n",
    "    \n",
    "    pre_train = train.copy()\n",
    "    pre_test = test.copy()\n",
    "    \n",
    "        \n",
    "    for df in [pre_train, pre_test]:\n",
    "        \n",
    "        for window in [50]:\n",
    "            \n",
    "            # roll backwards\n",
    "            df['S1mean_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "            df['S1mean_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "            df['S1mean_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "            df['S1mean_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).mean())\n",
    "\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "            df['S1std_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).std())\n",
    "\n",
    "            df['S1var_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "            df['S1var_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "            df['S1var_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "            df['S1var_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).var())\n",
    "\n",
    "            df['S1min_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            df['S1min_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            df['S1min_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            df['S1min_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).min())\n",
    "            \n",
    "            \n",
    "            df['S1max_t' + str(window) + '_S1'] = df.groupby(['id'])['S1'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "            df['S1max_t' + str(window) + '_S2'] = df.groupby(['id'])['S2'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "            df['S1max_t' + str(window) + '_S3'] = df.groupby(['id'])['S3'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "            df['S1max_t' + str(window) + '_S4'] = df.groupby(['id'])['S4'].transform(lambda x: x.shift(1).rolling(window).max())\n",
    "\n",
    "            min_max = (df['S1'] - df['S1min_t' + str(window) + '_S1']) / (df['S1max_t' + str(window) + '_S1'] - df['S1min_t' + str(window) + '_S1'])\n",
    "            df['norm_t' + str(window) + '_S1'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S1']) - np.ceil(df['S1min_t' + str(window) + '_S1']))\n",
    "            min_max = (df['S2'] - df['S1min_t' + str(window) + '_S2']) / (df['S1max_t' + str(window) + '_S2'] - df['S1min_t' + str(window) + '_S2'])\n",
    "            df['norm_t' + str(window) + '_S2'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S2']) - np.ceil(df['S1min_t' + str(window) + '_S2']))\n",
    "            min_max = (df['S3'] - df['S1min_t' + str(window) + '_S3']) / (df['S1max_t' + str(window) + '_S3'] - df['S1min_t' + str(window) + '_S3'])\n",
    "            df['norm_t' + str(window) + '_S3'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S3']) - np.ceil(df['S1min_t' + str(window) + '_S3']))\n",
    "            min_max = (df['S4'] - df['S1min_t' + str(window) + '_S4']) / (df['S1max_t' + str(window) + '_S4'] - df['S1min_t' + str(window) + '_S4'])\n",
    "            df['norm_t' + str(window) + '_S4'] = min_max * (np.floor(df['S1max_t' + str(window) + '_S4']) - np.ceil(df['S1min_t' + str(window) + '_S4']))\n",
    "\n",
    "    del train, test, min_max\n",
    "    \n",
    "    \n",
    "    return pre_train.fillna(0), pre_test.fillna(0)\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    \n",
    "    signal_col = [f'S{x}' for x in range(1, 5)]\n",
    "    for col in signal_col:\n",
    "        train_input_mean = train[col].mean()\n",
    "        train_input_sigma = train[col].std()\n",
    "        train[col] = (train[col] - train_input_mean) / train_input_sigma\n",
    "        test[col] = (test[col] - train_input_mean) / train_input_sigma\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    \n",
    "    signal_col = [f'S{x}' for x in range(1, 5)]\n",
    "    for col in signal_col:\n",
    "        for window in windows:\n",
    "            df[col + '_pos_' + str(window)] = df.groupby('group')[col].shift(window).fillna(0)\n",
    "            df[col + '_shift_neg_' + str(window)] = df.groupby('group')[col].shift(-1 * window).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df, batch_size):\n",
    "    # create batches\n",
    "    df = batching(df, batch_size = batch_size)\n",
    "    # create leads and lags (1, 2, 3 making them 6 features)\n",
    "    df = lag_with_pct_change(df, [1, 2])\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "# fillna with the mean and select features for training\n",
    "def feature_selection(train, test):\n",
    "    features = [col for col in train.columns if col not in ['index', 'id', 'group', 'X', 'Y', 'M', 'V', 'S1', 'S2', 'S3', 'S4']]\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "def split(train, test, GROUP_BATCH_SIZE=4000, SPLITS=5):\n",
    "    print('Reading Data Started...')\n",
    "    train, test = normalize(train, test)\n",
    "    train, test =  rolling_features(train, test)\n",
    "    print('Reading and Normalizing Data Completed')\n",
    "    print('Creating Features')\n",
    "    print('Feature Engineering Started...')\n",
    "    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n",
    "    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    train, test, features = feature_selection(train, test)\n",
    "    print('Feature Engineering Completed...')\n",
    "\n",
    "    target = ['X', 'Y', 'M', 'V']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=SPLITS)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])\n",
    "        new_splits.append(new_split)\n",
    "\n",
    "    train_tr = np.array(list(train.groupby('group').apply(lambda x: x[target].values))).astype(np.float32)\n",
    "    train = np.array(list(train.groupby('group').apply(lambda x: x[features].values)))\n",
    "    \n",
    "    \n",
    "    test = np.array(list(test.groupby('group').apply(lambda x: x[features].values)))\n",
    "    print(train.shape, test.shape, train_tr.shape)\n",
    "    return train, test, train_tr, new_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_tr, new_splits = split(train, test, GROUP_BATCH_SIZE=GBS, SPLITS=SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:, :200, :]\n",
    "test = test[:, :200, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tr = train_tr[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IronDataset(Dataset):\n",
    "    def __init__(self, data, labels, training=True, transform=False):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.training = training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        data = self.data[idx]\n",
    "\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return [data.astype(np.float32), labels.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qyq90ZzB2lnk"
   },
   "source": [
    "## 4. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rel1AkRP2lnm"
   },
   "outputs": [],
   "source": [
    "def kaeri_metric(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: KAERI metric\n",
    "    '''\n",
    "    \n",
    "    return 0.5 * E1(y_true, y_pred) + 0.5 * E2(y_true, y_pred)\n",
    "\n",
    "\n",
    "### E1과 E2는 아래에 정의됨 ###\n",
    "\n",
    "def E1(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: distance error normalized with 2e+04\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,:2], np.array(y_pred)[:,:2]\n",
    "    \n",
    "    return np.mean(np.sum(np.square(_t - _p), axis = 1) / 2e+04)\n",
    "\n",
    "\n",
    "def E2(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,2:], np.array(y_pred)[:,2:]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.sum(np.square((_t - _p) / (_t + 1e-06)), axis = 1))\n",
    "\n",
    "\n",
    "def E2M(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,2], np.array(y_pred)[:,2]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))\n",
    "\n",
    "def E2V(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = np.array(y_true)[:,-1], np.array(y_pred)[:,-1]\n",
    "    \n",
    "    \n",
    "    return np.mean(np.square((_t - _p) / (_t + 1e-06)))\n",
    "\n",
    "def torch_kaeri_metric(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: KAERI metric\n",
    "    '''\n",
    "    \n",
    "    return 0.5 * torch_E1(y_true, y_pred) + 0.5 * torch_E2(y_true, y_pred)\n",
    "\n",
    "\n",
    "### E1과 E2는 아래에 정의됨 ###\n",
    "\n",
    "def torch_E1(y_true, y_pred):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    return: distance error normalized with 2e+04\n",
    "    '''\n",
    "    _t, _p = y_true[:,:2], y_pred[:,:2]\n",
    "    return torch.mean((_t - _p)**2 / 2e+04)\n",
    "def torch_E2(y_true, y_pred, ):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    _t, _p = y_true[:,2:], y_pred[:,2:]\n",
    "    return torch.mean(((_t - _p) / (_t + 1e-06))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wave_Block(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_channels,out_channels,dilation_rates):\n",
    "        super(Wave_Block,self).__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(nn.Conv1d(in_channels,out_channels,kernel_size=1))\n",
    "        dilation_rates = [2**i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.filter_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n",
    "            self.gate_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n",
    "            self.convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=1))\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x = self.convs[0](x)\n",
    "        res = x\n",
    "        for i in range(self.num_rates):\n",
    "            x = torch.tanh(self.filter_convs[i](x))*torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = self.convs[i+1](x)\n",
    "            res = torch.add(res, x)\n",
    "        return res\n",
    "\n",
    "\n",
    "class Classifier_wave(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # For normal input\n",
    "        self.wave_block1 = Wave_Block(38,64,12)\n",
    "        self.bn_1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.wave_block2 = Wave_Block(64,64,8)\n",
    "        self.bn_2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.wave_block3 = Wave_Block(64,64,4)\n",
    "        self.bn_3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.wave_block4 = Wave_Block(64,128,1)\n",
    "        self.bn_4 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc = nn.Linear(128, 4)\n",
    "        \n",
    "        \n",
    "\n",
    "    def flip(self, x, dim):\n",
    "        dim = x.dim() + dim if dim < 0 else dim\n",
    "        return x[tuple(slice(None, None) if i != dim\n",
    "                 else torch.arange(x.size(i)-1, -1, -1).long()\n",
    "                 for i in range(x.dim()))]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # forward input\n",
    "        x = self.wave_block1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = self.wave_block2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.wave_block3(x)\n",
    "        x = self.bn_3(x)\n",
    "        x = self.wave_block4(x)\n",
    "        x = self.bn_4(x)\n",
    "#         x = x.permute(0, 2, 1)\n",
    "        x = x.mean(dim=2)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Classifier_res(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(1, 3, (3, 1))\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model = list(model.children())[:-1]\n",
    "        model.append(nn.Conv2d(512, 4, 1))\n",
    "        self.net = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # CNN model\n",
    "        x = self.conv_1(x)\n",
    "        x = self.net(x)\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
    "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
    "        self.counter, self.best_score = 0, None\n",
    "        self.is_maximize = is_maximize\n",
    "\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "                (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            self.best_score, self.counter = score, 0\n",
    "            return 1\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return 2\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V21KUtyl2lnu"
   },
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNU-ahud2lnv"
   },
   "outputs": [],
   "source": [
    "def do_train(train, test, do_wave):\n",
    "    \n",
    "    if not do_wave:\n",
    "        train = np.expand_dims(train, 1)\n",
    "        test = np.expand_dims(test, 1)\n",
    "        \n",
    "    test_y = np.zeros([test.shape[0], 4])\n",
    "    test_dataset = IronDataset(test, test_y)\n",
    "    test_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_preds_all = np.zeros([test.shape[0], 4])\n",
    "\n",
    "\n",
    "    oof_score = []\n",
    "    for index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n",
    "        print(\"Fold : {}\".format(index))\n",
    "        train_dataset = IronDataset(train[train_index], train_tr[train_index])\n",
    "        train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=6, pin_memory=True)\n",
    "\n",
    "        valid_dataset = IronDataset(train[val_index], train_tr[val_index])\n",
    "        valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        it = 0\n",
    "        if do_wave:\n",
    "            model = Classifier_wave()\n",
    "        else:\n",
    "            model = Classifier_res()\n",
    "            \n",
    "        model = model.cuda()\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=200, is_maximize=False,\n",
    "                                       checkpoint_path=os.path.join(outdir, \"cnn_fold_{}_iter_{}.pt\".format(index, it)))\n",
    "\n",
    "        weight = None\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', patience=5, factor=0.9)\n",
    "\n",
    "        avg_train_losses, avg_valid_losses = [], []\n",
    "\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('**********************************')\n",
    "            print(\"Folder : {} Epoch : {}\".format(index, epoch))\n",
    "            print(\"Curr learning_rate: {:0.9f}\".format(opt.param_groups[0]['lr']))\n",
    "            train_losses, valid_losses = [], []\n",
    "            tr_loss_cls_item, val_loss_cls_item = [], []\n",
    "\n",
    "\n",
    "            model.train()  # prep model for training\n",
    "            train_preds, train_true = torch.Tensor([]).cuda(), torch.Tensor([]).cuda()\n",
    "\n",
    "            for (x, y) in train_dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "\n",
    "                predictions = model(x)\n",
    "\n",
    "                loss = torch_kaeri_metric(y, predictions)\n",
    "\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                train_losses.append(loss.item())\n",
    "                train_true = torch.cat([train_true, y], 0)\n",
    "                train_preds = torch.cat([train_preds, predictions], 0)\n",
    "\n",
    "\n",
    "            model.eval()  # prep model for evaluation\n",
    "            val_preds, val_true = torch.Tensor([]).cuda(), torch.Tensor([]).cuda()\n",
    "            print('EVALUATION')\n",
    "            with torch.no_grad():\n",
    "                for (x, y) in valid_dataloader:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "\n",
    "                    predictions = model(x)\n",
    "\n",
    "                    loss = torch_kaeri_metric(y, predictions)\n",
    "                    valid_losses.append(loss.item())\n",
    "\n",
    "                    val_true = torch.cat([val_true, y], 0)\n",
    "                    val_preds = torch.cat([val_preds, predictions], 0)\n",
    "\n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            avg_train_losses.append(train_loss)\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "\n",
    "            if epoch % 5 ==0:\n",
    "                print(\"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "            train_score = kaeri_metric(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy())\n",
    "\n",
    "            val_score = kaeri_metric(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy())\n",
    "\n",
    "            schedular.step(valid_loss) \n",
    "            if epoch % 5 ==0:\n",
    "                print(\"train_metric: {:0.6f}, valid_metric: {:0.6f}\".format(train_score, val_score))\n",
    "            res = early_stopping(valid_loss, model) #  - val_score\n",
    "            if  res == 2:\n",
    "                print(\"Early Stopping\")\n",
    "                print('folder %d global best val max metric model score %f' % (index, early_stopping.best_score))\n",
    "                break\n",
    "            elif res == 1:\n",
    "                print('save folder %d global val max metric model score %f loss %f' % (index, val_score, valid_loss))\n",
    "        print('Folder {} finally best global max metric score is {}'.format(index, early_stopping.best_score))\n",
    "        oof_score.append(round(early_stopping.best_score, 6))\n",
    "\n",
    "        model.load_state_dict(torch.load(os.path.join(outdir, \"cnn_fold_{}_iter_{}.pt\".format(index, it))))\n",
    "        model.eval()\n",
    "        pred_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(test_dataloader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "                predictions = model(x)\n",
    "                pred_list.append(predictions.cpu().numpy())\n",
    "            test_preds = np.vstack(pred_list)\n",
    "            test_preds_all += test_preds / SPLIT\n",
    "    print('all folder score is:%s'%str(oof_score))\n",
    "    print('OOF mean score is: %f'% (sum(oof_score)/len(oof_score)))\n",
    "    print('Generate submission.............')\n",
    "    submission_csv_path = './data/KAERI_dataset/sample_submission.csv'\n",
    "    sub = pd.read_csv(submission_csv_path)\n",
    "    sub.iloc[:, 1:] = test_preds_all\n",
    "    \n",
    "    if do_wave:\n",
    "        sub.to_csv(\"./wavenet_preds.csv\", index=False)\n",
    "    else:\n",
    "        sub.to_csv(\"./res18_preds.csv\", index=False)\n",
    "        \n",
    "    print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train(train, test, do_wave=True)\n",
    "do_train(train, test, do_wave=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BNtFKiZ2ln6"
   },
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMEdF_8z2ln7"
   },
   "outputs": [],
   "source": [
    "def post_pro(sub):\n",
    "    sub.loc[sub.X < -400, 'X'] = -400\n",
    "    sub.loc[sub.X > 400, 'X'] = 400\n",
    "\n",
    "    sub.loc[sub.Y < -400, 'Y'] = -400\n",
    "    sub.loc[sub.Y > 400, 'Y'] = 400\n",
    "\n",
    "    sub.loc[sub.M < 25, 'M'] = 25\n",
    "    sub.loc[sub.M > 175, 'M'] = 175\n",
    "\n",
    "    sub.loc[sub.V < 0.2, 'V'] = 0.2\n",
    "    sub.loc[sub.V > 1.0, 'V'] = 1.0\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_submit = pd.read_csv('./cnn_nf32_pred.csv')\n",
    "cnn_flip_submit = pd.read_csv('./cnn_nf32_flip_pred.csv')\n",
    "\n",
    "cnn_nf16_submit = pd.read_csv('./cnn_nf16_pred.csv')\n",
    "cnn_nf16_flip_submit = pd.read_csv('./cnn_nf16_flip_pred.csv')\n",
    "\n",
    "cnn_nf16_gap_submit = pd.read_csv('./cnn_nf16_gap_pred.csv')\n",
    "cnn_nf16_gap_flip_submit = pd.read_csv('./cnn_nf16_gap_flip_pred.csv')\n",
    "\n",
    "cnn_gap_submit = pd.read_csv('./cnn_nf32_gap_pred.csv')\n",
    "cnn_gap_flip_submit = pd.read_csv('./cnn_nf32_gap_flip_pred.csv')\n",
    "\n",
    "cnn_375_submit = pd.read_csv('./cnn_nf16_375_pred.csv')\n",
    "cnn_gap_375_submit = pd.read_csv('./cnn_nf16_gap_375_pred.csv')\n",
    "\n",
    "wavenet_submit = pd.read_csv('./wavenet_preds.csv')\n",
    "res18_submit = pd.read_csv('./res18_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_submit = post_pro(cnn_submit)\n",
    "cnn_flip_submit = post_pro(cnn_flip_submit)\n",
    "\n",
    "cnn_nf16_submit = post_pro(cnn_nf16_submit)\n",
    "cnn_nf16_flip_submit = post_pro(cnn_nf16_flip_submit)\n",
    "\n",
    "cnn_nf16_gap_submit = post_pro(cnn_nf16_gap_submit)\n",
    "cnn_nf16_gap_flip_submit = post_pro(cnn_nf16_gap_flip_submit)\n",
    "\n",
    "cnn_gap_submit = post_pro(cnn_gap_submit)\n",
    "cnn_gap_flip_submit = post_pro(cnn_gap_flip_submit)\n",
    "\n",
    "cnn_375_submit = post_pro(cnn_375_submit)\n",
    "cnn_gap_375_submit = post_pro(cnn_gap_375_submit)\n",
    "\n",
    "wavenet_submit = post_pro(wavenet_submit)\n",
    "res18_submit = post_pro(res18_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/KAERI_dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Y\n",
    "X_Y_pred = (cnn_submit.iloc[:, 1:3].values + cnn_flip_submit.iloc[:, 1:3].values + cnn_nf16_submit.iloc[:, 1:3].values + \n",
    "       cnn_nf16_flip_submit.iloc[:, 1:3].values + cnn_nf16_gap_submit.iloc[:, 1:3].values + cnn_nf16_gap_flip_submit.iloc[:, 1:3].values + \n",
    "        cnn_gap_submit.iloc[:, 1:3].values + cnn_gap_flip_submit.iloc[:, 1:3].values + \n",
    "        cnn_375_submit.iloc[:, 1:3].values + cnn_gap_375_submit.iloc[:, 1:3].values + \n",
    "        res18_submit.iloc[:, 1:3].values + wavenet_submit.iloc[:, 1:3].values\n",
    "       ) / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M V\n",
    "M_V_pred = (cnn_submit.iloc[:, 3:].values + cnn_flip_submit.iloc[:, 3:].values + cnn_nf16_submit.iloc[:, 3:].values + \n",
    "       cnn_nf16_flip_submit.iloc[:, 3:].values + cnn_nf16_gap_submit.iloc[:, 3:].values + cnn_nf16_gap_flip_submit.iloc[:, 3:].values + \n",
    "        cnn_gap_submit.iloc[:, 3:].values + cnn_gap_flip_submit.iloc[:, 3:].values\n",
    "       ) / 8\n",
    "M_V_pred = M_V_pred * 0.5 + wavenet_submit.iloc[:, 3:].values * 0.2 + res18_submit.iloc[:, 3:].values * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:, 1:3] = X_Y_pred\n",
    "sub.iloc[:, 3:] = M_V_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('./result_pred.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KAERI_source_code.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
